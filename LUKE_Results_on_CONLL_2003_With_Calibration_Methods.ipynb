{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TtMnpKyGATY-"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m4DcWi3EFaxU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRs4Wyghwse7"
      },
      "source": [
        "# Reproducing experimental results of LUKE on CoNLL-2003 Using Hugging Face Transformers and applying several calibration methods\n",
        "\n",
        "The following notebook is an adapted version of one provided by the developers of LUKE, used to replicate their experiment on an existing dataset. linked here: https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18lGoU-SmTK8",
        "outputId": "5694f92d-cb25-4b5c-8297-cb965c1228cc"
      },
      "source": [
        "# Currently, LUKE is only available on the master branch\n",
        "!pip install seqeval git+https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-hkuy2940\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-hkuy2940\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 80377eb018c077dba434bc8e7912bcaed3a64d09\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (0.19.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.36.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.36.0.dev0) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBERtWDCnqXO"
      },
      "source": [
        "import unicodedata\n",
        "\n",
        "import numpy as np\n",
        "import seqeval.metrics\n",
        "import spacy\n",
        "import torch\n",
        "from tqdm import tqdm, trange\n",
        "from transformers import LukeTokenizer, LukeForEntitySpanClassification"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYBJqG6JBNes"
      },
      "source": [
        "## Loading the dataset\n",
        "\n",
        "The test set of the CoNLL-2003 dataset (eng.testb) is placed in the current directory and loaded using `load_examples` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLzX8LIS127b",
        "outputId": "7fa3fb13-04a1-4efa-8970-629fe8bb8c5c"
      },
      "source": [
        "# Download the testb set of the CoNLL-2003 dataset\n",
        "!wget https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testb"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-09 03:46:16--  https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testb\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 748096 (731K) [text/plain]\n",
            "Saving to: ‘eng.testb.1’\n",
            "\n",
            "\reng.testb.1           0%[                    ]       0  --.-KB/s               \reng.testb.1         100%[===================>] 730.56K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-12-09 03:46:16 (19.6 MB/s) - ‘eng.testb.1’ saved [748096/748096]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gfWDJsgBjGw"
      },
      "source": [
        "## Loading the fine-tuned model and tokenizer\n",
        "\n",
        "We construct the model and tokenizer using the [fine-tuned model checkpoint](https://huggingface.co/studio-ousia/luke-large-finetuned-conll-2003)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9bXAEPZp0ZT",
        "outputId": "0d1d779b-7beb-44ec-f7d9-bc9830b83f2a"
      },
      "source": [
        "# Load the model checkpoint\n",
        "model = LukeForEntitySpanClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\")\n",
        "model.eval()\n",
        "model.to(\"cuda\")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at studio-ousia/luke-large-finetuned-conll-2003 were not used when initializing LukeForEntitySpanClassification: ['luke.embeddings.position_ids']\n",
            "- This IS expected if you are initializing LukeForEntitySpanClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LukeForEntitySpanClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz-nZAmVBopz"
      },
      "source": [
        "## Loading the dataset\n",
        "\n",
        "First, the documents in the dataset is loaded using the `load_documents` function. This function outputs the list of dicts which have the following three keys:\n",
        "* `words`: the sequence of words\n",
        "* `labels`: the sequence of gold-standard NER labels (`\"MISC\"`, `\"PER\"`, `\"ORG\"`, or `\"LOC\"`)\n",
        "* `sentence_boundaries`: positions of sentence boundaries in the word sequence\n",
        "\n",
        "The `load_examples` function creates a batch instance for each sentence in a document.\n",
        "The model addresses the task by classifying all possible entity spans in a sentence into `[\"NIL\", \"MISC\", \"PER\", \"ORG\", \"LOC\"]`, where `\"NIL\"` represents that the span is not an entity name (see Section 4.3 in the [original paper](https://arxiv.org/abs/2010.01057)).\n",
        "Here, we create the list of all possible entity spans (character-based start and entity positions) in a sentence.\n",
        "Specifically, this function returns the list of dicts with the following four keys:\n",
        "* `text`: text\n",
        "* `words`: the sequence of words\n",
        "* `entity_spans`: the list of possible entity spans (character-based start and end positions in the `text`)\n",
        "* `original_word_spans`: the list of corresponding spans of `entity_spans` in the word sequence\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-v01ix-i2FS"
      },
      "source": [
        "def load_documents(dataset_file):\n",
        "    documents = []\n",
        "    words = []\n",
        "    labels = []\n",
        "    sentence_boundaries = []\n",
        "    with open(dataset_file) as f:\n",
        "        for line in f:\n",
        "            line = line.rstrip()\n",
        "            if line.startswith(\"-DOCSTART\"):\n",
        "                if words:\n",
        "                    documents.append(dict(\n",
        "                        words=words,\n",
        "                        labels=labels,\n",
        "                        sentence_boundaries=sentence_boundaries\n",
        "                    ))\n",
        "                    words = []\n",
        "                    labels = []\n",
        "                    sentence_boundaries = []\n",
        "                continue\n",
        "\n",
        "            if not line:\n",
        "                if not sentence_boundaries or len(words) != sentence_boundaries[-1]:\n",
        "                    sentence_boundaries.append(len(words))\n",
        "            else:\n",
        "                items = line.split(\" \")\n",
        "                words.append(items[0])\n",
        "                labels.append(items[-1])\n",
        "\n",
        "    if words:\n",
        "        documents.append(dict(\n",
        "            words=words,\n",
        "            labels=labels,\n",
        "            sentence_boundaries=sentence_boundaries\n",
        "        ))\n",
        "\n",
        "    return documents\n",
        "\n",
        "\n",
        "def load_examples(documents):\n",
        "    examples = []\n",
        "    max_token_length = 510\n",
        "    max_mention_length = 30\n",
        "\n",
        "    for document in tqdm(documents):\n",
        "        words = document[\"words\"]\n",
        "        subword_lengths = [len(tokenizer.tokenize(w)) for w in words]\n",
        "        total_subword_length = sum(subword_lengths)\n",
        "        sentence_boundaries = document[\"sentence_boundaries\"]\n",
        "\n",
        "        for i in range(len(sentence_boundaries) - 1):\n",
        "            sentence_start, sentence_end = sentence_boundaries[i:i+2]\n",
        "            if total_subword_length <= max_token_length:\n",
        "                # if the total sequence length of the document is shorter than the\n",
        "                # maximum token length, we simply use all words to build the sequence\n",
        "                context_start = 0\n",
        "                context_end = len(words)\n",
        "            else:\n",
        "                # if the total sequence length is longer than the maximum length, we add\n",
        "                # the surrounding words of the target sentence　to the sequence until it\n",
        "                # reaches the maximum length\n",
        "                context_start = sentence_start\n",
        "                context_end = sentence_end\n",
        "                cur_length = sum(subword_lengths[context_start:context_end])\n",
        "                while True:\n",
        "                    if context_start > 0:\n",
        "                        if cur_length + subword_lengths[context_start - 1] <= max_token_length:\n",
        "                            cur_length += subword_lengths[context_start - 1]\n",
        "                            context_start -= 1\n",
        "                        else:\n",
        "                            break\n",
        "                    if context_end < len(words):\n",
        "                        if cur_length + subword_lengths[context_end] <= max_token_length:\n",
        "                            cur_length += subword_lengths[context_end]\n",
        "                            context_end += 1\n",
        "                        else:\n",
        "                            break\n",
        "\n",
        "            text = \"\"\n",
        "            for word in words[context_start:sentence_start]:\n",
        "                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n",
        "                    text = text.rstrip()\n",
        "                text += word\n",
        "                text += \" \"\n",
        "\n",
        "            sentence_words = words[sentence_start:sentence_end]\n",
        "            sentence_subword_lengths = subword_lengths[sentence_start:sentence_end]\n",
        "\n",
        "            word_start_char_positions = []\n",
        "            word_end_char_positions = []\n",
        "            for word in sentence_words:\n",
        "                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n",
        "                    text = text.rstrip()\n",
        "                word_start_char_positions.append(len(text))\n",
        "                text += word\n",
        "                word_end_char_positions.append(len(text))\n",
        "                text += \" \"\n",
        "\n",
        "            for word in words[sentence_end:context_end]:\n",
        "                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n",
        "                    text = text.rstrip()\n",
        "                text += word\n",
        "                text += \" \"\n",
        "            text = text.rstrip()\n",
        "\n",
        "            entity_spans = []\n",
        "            original_word_spans = []\n",
        "            for word_start in range(len(sentence_words)):\n",
        "                for word_end in range(word_start, len(sentence_words)):\n",
        "                    if sum(sentence_subword_lengths[word_start:word_end + 1]) <= max_mention_length:\n",
        "                        entity_spans.append(\n",
        "                            (word_start_char_positions[word_start], word_end_char_positions[word_end])\n",
        "                        )\n",
        "                        original_word_spans.append(\n",
        "                            (word_start, word_end + 1)\n",
        "                        )\n",
        "\n",
        "            examples.append(dict(\n",
        "                text=text,\n",
        "                words=sentence_words,\n",
        "                entity_spans=entity_spans,\n",
        "                original_word_spans=original_word_spans,\n",
        "            ))\n",
        "\n",
        "    return examples\n",
        "\n",
        "\n",
        "def is_punctuation(char):\n",
        "    cp = ord(char)\n",
        "    if (cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126):\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"P\"):\n",
        "        return True\n",
        "    return False"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H6us8Unl231",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74e2da87-9452-4d08-ce83-08b0fffa3b89"
      },
      "source": [
        "test_documents = load_documents(\"eng.testb\")\n",
        "test_examples = load_examples(test_documents)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231/231 [00:05<00:00, 42.40it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eomnOqQKDNUX"
      },
      "source": [
        "## Measuring performance\n",
        "\n",
        "We classify all possible entity spans in the test set, exclude all spans classified into the `NIL` type, and greedily select a span from the remaining spans based on the logit of its predicted entity type in descending order.\n",
        "Due to  minor differences in processing, the reproduced performance is slightly lower than the performance reported in the [original paper](https://arxiv.org/abs/2010.01057) (approximately 0.1 F1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGnyU7rGnNg2",
        "outputId": "89ab588c-0d6b-48e3-d7a0-44026637c452"
      },
      "source": [
        "batch_size = 2\n",
        "all_logits = []\n",
        "\n",
        "for batch_start_idx in trange(0, len(test_examples), batch_size):\n",
        "    batch_examples = test_examples[batch_start_idx:batch_start_idx + batch_size]\n",
        "    texts = [example[\"text\"] for example in batch_examples]\n",
        "    entity_spans = [example[\"entity_spans\"] for example in batch_examples]\n",
        "\n",
        "    inputs = tokenizer(texts, entity_spans=entity_spans, return_tensors=\"pt\", padding=True)\n",
        "    inputs = inputs.to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    all_logits.extend(outputs.logits.tolist())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1727/1727 [09:13<00:00,  3.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcNmRnjne339"
      },
      "source": [
        "final_labels = [label for document in test_documents for label in document[\"labels\"]]\n",
        "\n",
        "calibration_logits = []\n",
        "\n",
        "uncalibrated_predictions = []\n",
        "\n",
        "logits_as_one = np.array(all_logits[0])\n",
        "\n",
        "golds_as_one = np.array([])\n",
        "\n",
        "hist_bins = []\n",
        "\n",
        "for example_index, example in enumerate(test_examples):\n",
        "    logits = all_logits[example_index]\n",
        "    max_logits = np.max(logits, axis=1)\n",
        "    max_indices = np.argmax(logits, axis=1)\n",
        "    original_spans = example[\"original_word_spans\"]\n",
        "    predictions = []\n",
        "\n",
        "    for logit, index, span, logs in zip(max_logits, max_indices, original_spans, logits):\n",
        "        if index != 0:  # the span is not NIL\n",
        "            predictions.append((logit, span, model.config.id2label[index], logs))\n",
        "\n",
        "    # construct an IOB2 label sequence\n",
        "    predicted_sequence = [\"O\"] * len(example[\"words\"])\n",
        "    predicted_logits = [[1, 0, 0, 0, 0]] * len(example[\"words\"])\n",
        "\n",
        "    for _, span, label, logs in sorted(predictions, key=lambda o: o[0], reverse=True):\n",
        "        if all([o == \"O\" for o in predicted_sequence[span[0] : span[1]]]):\n",
        "            predicted_sequence[span[0]] = \"B-\" + label\n",
        "            predicted_logits[span[0]] = logs\n",
        "            if span[1] - span[0] > 1:\n",
        "                predicted_sequence[span[0] + 1 : span[1]] = [\"I-\" + label] * (span[1] - span[0] - 1)\n",
        "                predicted_logits[span[0] + 1: span[1]] = [logs] * (span[1] - span[0] - 1)\n",
        "\n",
        "    uncalibrated_predictions += predicted_sequence\n",
        "    calibration_logits += predicted_logits\n",
        "\n",
        "translation = {\"O\": \"NIL\",\"I-LOC\": \"LOC\",\"I-ORG\": \"ORG\",\"I-PER\": \"PER\",\"I-MISC\": \"MISC\",\"B-LOC\": \"LOC\",\"B-ORG\": \"ORG\",\"B-PER\": \"PER\",\"B-MISC\": \"MISC\"}\n",
        "\n",
        "gold_labels = [model.config.label2id[translation[label]] for label in final_labels]\n",
        "\n",
        "from scipy.special import softmax\n",
        "\n",
        "probs_for_binning = softmax(calibration_logits, axis = 1)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExpectedCalibrationError():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.conf = []  # Initiate confidence list\n",
        "\n",
        "\n",
        "    def _get_conf_acc(self, lower_bound, upper_bound, probs, true):\n",
        "        # Filter labels within probability range\n",
        "\n",
        "        arg_max = np.argmax(probs, axis = 1)\n",
        "        max = np.max(probs, axis = 1)\n",
        "\n",
        "        acc_filtered = [x[0] == x[2] for x in zip(true, max, arg_max) if x[1] > lower_bound and x[1] <= upper_bound]\n",
        "\n",
        "        conf_filtered = [x[1] for x in zip(true, max, arg_max) if x[1] > lower_bound and x[1] <= upper_bound]\n",
        "        nr_elems = len(acc_filtered)\n",
        "\n",
        "        if nr_elems < 1:\n",
        "            return 0, 0, 0\n",
        "        else:\n",
        "            acc = np.sum(acc_filtered)/nr_elems  # Sums positive classes\n",
        "            conf = np.sum(conf_filtered)/nr_elems # Sums maximum probabilities\n",
        "            return conf, acc, nr_elems\n",
        "\n",
        "\n",
        "    def ECE(self, probs, true):\n",
        "\n",
        "        self.upper_bounds = np.histogram_bin_edges(probs, bins = 100, range = (0, 1))[1:].tolist() + [1]\n",
        "        conf = []\n",
        "        acc = []\n",
        "        bin_sizes = []\n",
        "\n",
        "        # Go through intervals and add confidence to list\n",
        "        for i, conf_thresh in enumerate(self.upper_bounds):\n",
        "            temp_conf, temp_acc, bin_size = self._get_conf_acc(self.upper_bounds[i - 1] if i > 0 else 0, conf_thresh, probs = probs, true = true)\n",
        "            conf.append(temp_conf)\n",
        "            acc.append(temp_acc)\n",
        "            bin_sizes.append(bin_size)\n",
        "\n",
        "        len = np.sum(bin_sizes)\n",
        "\n",
        "        ECE = np.sum([size * abs(accuracy - confidence) for accuracy, confidence, size in zip(acc, conf, bin_sizes)])/len\n",
        "\n",
        "        return ECE"
      ],
      "metadata": {
        "id": "JRXbGaYawb9O"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFEYKr9tuk36",
        "outputId": "a36efed2-e688-4aea-cda4-be5a8d4ddc07"
      },
      "source": [
        "print(\"ECE:\")\n",
        "print(ExpectedCalibrationError().ECE(probs_for_binning, gold_labels))\n",
        "\n",
        "print(seqeval.metrics.classification_report([final_labels], [uncalibrated_predictions], digits=4))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ECE:\n",
            "0.4957551166667858\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.9558    0.9478    0.9518      1666\n",
            "        MISC     0.8553    0.8688    0.8620       701\n",
            "         ORG     0.9287    0.9496    0.9391      1647\n",
            "         PER     0.9683    0.9719    0.9701      1602\n",
            "\n",
            "   micro avg     0.9386    0.9453    0.9420      5616\n",
            "   macro avg     0.9270    0.9345    0.9307      5616\n",
            "weighted avg     0.9389    0.9453    0.9421      5616\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Measuring Performance With Calibration"
      ],
      "metadata": {
        "id": "M2nsvD-pScu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use several different calibration methods in post-processing as described in the paper: https://arxiv.org/pdf/1706.04599.pdf"
      ],
      "metadata": {
        "id": "PoQJ24svSjzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Histogram Binning"
      ],
      "metadata": {
        "id": "U_EGIzbsSxqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HistogramBinning():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.conf = []\n",
        "\n",
        "\n",
        "    def confidence(self, lower_bound, upper_bound, probs, true):\n",
        "        filtered = [x[0] for x in zip(true, probs) if x[1] > lower_bound and x[1] <= upper_bound]\n",
        "        nr_elems = len(filtered)\n",
        "\n",
        "        if nr_elems < 1:\n",
        "            return 0\n",
        "        else:\n",
        "            conf = np.sum(filtered)/nr_elems\n",
        "            return conf\n",
        "\n",
        "\n",
        "    def train(self, probs, true):\n",
        "\n",
        "        self.upper_bounds = np.histogram_bin_edges(probs, bins = 50, range = (0, 0))[1:].tolist() + [1]\n",
        "\n",
        "        conf = []\n",
        "\n",
        "        # Got through intervals and add confidence to list\n",
        "        for i, conf_thresh in enumerate(self.upper_bounds):\n",
        "            temp_conf = self.confidence(self.upper_bounds[i - 1] if i > 0 else 0, conf_thresh, probs = probs, true = true)\n",
        "            conf.append(temp_conf)\n",
        "\n",
        "        self.conf = conf\n",
        "\n",
        "    def predict(self, probs):\n",
        "        new_prob = list(probs)\n",
        "        for i, prob in enumerate(probs):\n",
        "            idx = np.searchsorted(self.upper_bounds, prob)\n",
        "            item = self.conf[idx]\n",
        "            new_prob[i] = item\n",
        "        return new_prob"
      ],
      "metadata": {
        "id": "SzFrZ9N4oy0f"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_labels = [label for document in test_documents for label in document[\"labels\"]]\n",
        "\n",
        "final_predictions = []\n",
        "\n",
        "from scipy.special import softmax\n",
        "\n",
        "probs_for_binning = softmax(calibration_logits, axis = 1)"
      ],
      "metadata": {
        "id": "DS5kLYVSWvMW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_predictions = []\n",
        "\n",
        "hist_bins = []\n",
        "\n",
        "logit_bits = []\n",
        "\n",
        "translation = {\"O\": \"NIL\",\"I-LOC\": \"LOC\",\"I-ORG\": \"ORG\",\"I-PER\": \"PER\",\"I-MISC\": \"MISC\",\"B-LOC\": \"LOC\",\"B-ORG\": \"ORG\",\"B-PER\": \"PER\",\"B-MISC\": \"MISC\"}\n",
        "\n",
        "for i in range(len(calibration_logits[0])):\n",
        "  golds_as_one = [model.config.label2id[translation[label]] == i for label in final_labels]\n",
        "  bin = HistogramBinning()\n",
        "  bin.train(probs_for_binning[:20000, i], golds_as_one[:20000])\n",
        "  hist_bins.append(bin)\n",
        "\n",
        "for i in range(len(hist_bins)):\n",
        "  logit_bits.append(hist_bins[i].predict(probs_for_binning[:, i]))\n",
        "\n",
        "logits = np.stack(logit_bits, axis = 1)\n",
        "\n",
        "calibrated_predictions = []\n",
        "\n",
        "indices = np.argmax(logits, axis = 1)\n",
        "\n",
        "for index in indices:\n",
        "  label = model.config.id2label[index]\n",
        "  label = \"O\" if label == \"NIL\" else \"I-\" + label\n",
        "  calibrated_predictions.append(label)\n",
        "\n",
        "print(\"ECE:\")\n",
        "print(ExpectedCalibrationError().ECE(softmax(logits, axis = 1), gold_labels))\n",
        "\n",
        "print(seqeval.metrics.classification_report([final_labels], [calibrated_predictions], digits=4))"
      ],
      "metadata": {
        "id": "7O5Vvhs1g8ol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8db38e5-4753-4b48-9afd-b09d5bb07291"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ECE:\n",
            "0.5865472775873256\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.9569    0.9472    0.9520      1666\n",
            "        MISC     0.8621    0.8559    0.8590       701\n",
            "         ORG     0.9323    0.9454    0.9388      1647\n",
            "         PER     0.9793    0.9769    0.9781      1602\n",
            "\n",
            "   micro avg     0.9442    0.9437    0.9440      5616\n",
            "   macro avg     0.9327    0.9313    0.9320      5616\n",
            "weighted avg     0.9443    0.9437    0.9440      5616\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Isotonic Regression"
      ],
      "metadata": {
        "id": "x8wSit3tS5KE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.isotonic import IsotonicRegression"
      ],
      "metadata": {
        "id": "rMGxUzKBrVU4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_predictions = []\n",
        "\n",
        "regs = []\n",
        "\n",
        "logit_bits = []\n",
        "\n",
        "translation = {\"O\": \"NIL\",\"I-LOC\": \"LOC\",\"I-ORG\": \"ORG\",\"I-PER\": \"PER\",\"I-MISC\": \"MISC\",\"B-LOC\": \"LOC\",\"B-ORG\": \"ORG\",\"B-PER\": \"PER\",\"B-MISC\": \"MISC\"}\n",
        "\n",
        "for i in range(len(calibration_logits[0])):\n",
        "  golds_as_one = [model.config.label2id[translation[label]] == i for label in final_labels]\n",
        "  reg = IsotonicRegression()\n",
        "  reg.fit(probs_for_binning[20000:, i], golds_as_one[20000:])\n",
        "  regs.append(reg)\n",
        "\n",
        "for i in range(len(regs)):\n",
        "  logit_bits.append(regs[i].predict(probs_for_binning[:, i]))\n",
        "\n",
        "logits = np.stack(logit_bits, axis = 1)\n",
        "\n",
        "calibrated_predictions = []\n",
        "\n",
        "indices = np.argmax(logits, axis = 1)\n",
        "\n",
        "calibrated_predictions = []\n",
        "\n",
        "for index in indices:\n",
        "  label = model.config.id2label[index]\n",
        "  label = \"O\" if label == \"NIL\" else \"I-\" + label\n",
        "  calibrated_predictions.append(label)\n",
        "\n",
        "print(\"ECE:\")\n",
        "print(ExpectedCalibrationError().ECE(softmax(logits, axis = 1), gold_labels))\n",
        "\n",
        "print(seqeval.metrics.classification_report([final_labels], [calibrated_predictions], digits=4))"
      ],
      "metadata": {
        "id": "CFnVqdyDn187",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "369f9cb1-2e26-4b70-f0c3-017332842809"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ECE:\n",
            "0.5873015979076123\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.9575    0.9460    0.9517      1666\n",
            "        MISC     0.8620    0.8645    0.8632       701\n",
            "         ORG     0.9329    0.9460    0.9394      1647\n",
            "         PER     0.9824    0.9763    0.9793      1602\n",
            "\n",
            "   micro avg     0.9453    0.9444    0.9449      5616\n",
            "   macro avg     0.9337    0.9332    0.9334      5616\n",
            "weighted avg     0.9455    0.9444    0.9449      5616\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Matrix Scaling"
      ],
      "metadata": {
        "id": "gOzfYZBNS74c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "MkGx7IexCrtI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_predictions = []\n",
        "\n",
        "regs = []\n",
        "\n",
        "logit_bits = []\n",
        "\n",
        "translation = {\"O\": \"NIL\",\"I-LOC\": \"LOC\",\"I-ORG\": \"ORG\",\"I-PER\": \"PER\",\"I-MISC\": \"MISC\",\"B-LOC\": \"LOC\",\"B-ORG\": \"ORG\",\"B-PER\": \"PER\",\"B-MISC\": \"MISC\"}\n",
        "\n",
        "log_model = LogisticRegression(multi_class = 'ovr')\n",
        "\n",
        "gold_labels = [model.config.label2id[translation[label]] for label in final_labels]\n",
        "\n",
        "log_model = log_model.fit(calibration_logits, gold_labels)\n",
        "\n",
        "log_probs = log_model.predict_log_proba(calibration_logits)\n",
        "\n",
        "indices = np.argmax(log_probs, axis = 1)\n",
        "\n",
        "calibrated_predictions = []\n",
        "\n",
        "for index in indices:\n",
        "  label = model.config.id2label[index]\n",
        "  label = \"O\" if label == \"NIL\" else \"I-\" + label\n",
        "  calibrated_predictions.append(label)\n",
        "\n",
        "print(\"ECE:\")\n",
        "print(ExpectedCalibrationError().ECE(softmax(log_probs, axis = 1), gold_labels))\n",
        "\n",
        "print(seqeval.metrics.classification_report([final_labels], [calibrated_predictions], digits=4))"
      ],
      "metadata": {
        "id": "CdhqSOl3g9im",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bdcf530-4943-48a3-cd38-32e43ef6829d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ECE:\n",
            "0.0055017690648463905\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.9605    0.9334    0.9467      1666\n",
            "        MISC     0.8957    0.7960    0.8429       701\n",
            "         ORG     0.9344    0.9423    0.9383      1647\n",
            "         PER     0.9798    0.9707    0.9752      1602\n",
            "\n",
            "   micro avg     0.9508    0.9295    0.9400      5616\n",
            "   macro avg     0.9426    0.9106    0.9258      5616\n",
            "weighted avg     0.9503    0.9295    0.9394      5616\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Temperature Scaling"
      ],
      "metadata": {
        "id": "GxX8osZg8P3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_predictions = []\n",
        "\n",
        "regs = []\n",
        "\n",
        "logit_bits = []\n",
        "\n",
        "translation = {\"O\": \"NIL\",\"I-LOC\": \"LOC\",\"I-ORG\": \"ORG\",\"I-PER\": \"PER\",\"I-MISC\": \"MISC\",\"B-LOC\": \"LOC\",\"B-ORG\": \"ORG\",\"B-PER\": \"PER\",\"B-MISC\": \"MISC\"}\n",
        "\n",
        "gold_labels = [model.config.label2id[translation[label]] for label in final_labels]\n",
        "\n",
        "temp_logits = np.copy(calibration_logits)\n",
        "\n",
        "temperature = .1\n",
        "\n",
        "temp_logits = temp_logits / temperature\n",
        "\n",
        "indices = np.argmax(temp_logits, axis = 1)\n",
        "\n",
        "calibrated_predictions = []\n",
        "\n",
        "for index in indices:\n",
        "  label = model.config.id2label[index]\n",
        "  label = \"O\" if label == \"NIL\" else \"I-\" + label\n",
        "  calibrated_predictions.append(label)\n",
        "\n",
        "print(\"ECE:\")\n",
        "print(ExpectedCalibrationError().ECE(softmax(temp_logits, axis = 1), gold_labels))\n",
        "\n",
        "print(seqeval.metrics.classification_report([final_labels], [calibrated_predictions], digits=4))"
      ],
      "metadata": {
        "id": "ePD7ZmdoB48v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02455f65-0f32-4505-c3c2-8ac83e4a44e4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ECE:\n",
            "0.012199425990354187\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC     0.9552    0.9466    0.9509      1666\n",
            "        MISC     0.8623    0.8573    0.8598       701\n",
            "         ORG     0.9296    0.9460    0.9377      1647\n",
            "         PER     0.9781    0.9769    0.9775      1602\n",
            "\n",
            "   micro avg     0.9426    0.9439    0.9432      5616\n",
            "   macro avg     0.9313    0.9317    0.9315      5616\n",
            "weighted avg     0.9426    0.9439    0.9432      5616\n",
            "\n"
          ]
        }
      ]
    }
  ]
}